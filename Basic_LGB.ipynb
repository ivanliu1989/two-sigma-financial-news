{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\ndemo = False",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ede30eda5a6ff55f48660a84cad8cebd42468f02"
      },
      "cell_type": "code",
      "source": "(market_train_df, news_train_df) = env.get_training_data()\n\nif demo:\n    market_train_df = market_train_df.tail(100000)\n    news_train_df = news_train_df.tail(300000)\n\n# Save universe data for latter use\nuniverse = market_train_df['universe']\ntime = market_train_df['time']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "46d25a234837aad1c208697b5484986692e13901"
      },
      "cell_type": "code",
      "source": "def sigma_score(preds, valid_data):\n    labels = valid_data.get_label()\n    x_t = preds * labels # * df_valid['universe'] # -> Here we take out the 'universe' term because we already keep only those equals to 1.\n    # Here we take advantage of the fact that `labels` (used to calculate `x_t`)\n    # x_t = x_t.groupby(valid_data.params['extra_time']).sum()\n    score = x_t.mean() / x_t.std()\n\n    return 'sigma_score', score, True",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ddcc67d6d57d5b95d8845b742ca732a23bc1c02e"
      },
      "cell_type": "code",
      "source": "#%%time\n# code mostly takes from this kernel: https://www.kaggle.com/ashishpatel26/bird-eye-view-of-two-sigma-xgb\n\ndef feature_engineering(market_df,news_df):\n#     market_df['time'] = market_df.time.dt.date\n#     market_df['returnsOpenPrevRaw1_to_volume'] = market_df['returnsOpenPrevRaw1'] / market_df['volume']\n#     market_df['close_to_open'] = market_df['close'] / market_df['open']\n#     market_df['volume_to_mean'] = market_df['volume'] / market_df['volume'].mean()\n    \n#     news_df['time'] = news_df.time.dt.hour\n#     news_df['sourceTimestamp']= news_df.sourceTimestamp.dt.hour\n#     news_df['firstCreated'] = news_df.firstCreated.dt.date\n#     news_df['assetCodesLen'] = news_df['assetCodes'].map(lambda x: len(eval(x)))\n#     news_df['assetCodes'] = news_df['assetCodes'].map(lambda x: list(eval(x))[0])\n#     news_df['headlineLen'] = news_df['headline'].apply(lambda x: len(x))\n#     news_df['assetCodesLen'] = news_df['assetCodes'].apply(lambda x: len(x))\n#     news_df['asset_sentiment_count'] = news_df.groupby(['assetName', 'sentimentClass'])['time'].transform('count')\n#     news_df['asset_sentence_mean'] = news_df.groupby(['assetName', 'sentenceCount'])['time'].transform('mean')\n#     lbl = {k: v for v, k in enumerate(news_df['headlineTag'].unique())}\n#     news_df['headlineTagT'] = news_df['headlineTag'].map(lbl)\n#     kcol = ['firstCreated', 'assetCodes']\n#     news_df = news_df.groupby(kcol, as_index=False).mean()\n\n#     market_df = pd.merge(market_df, news_df, how='left', left_on=['time', 'assetCode'], \n#                             right_on=['firstCreated', 'assetCodes'])\n\n#     lbl = {k: v for v, k in enumerate(market_df['assetCode'].unique())}\n#     market_df['assetCodeT'] = market_df['assetCode'].map(lbl)\n    \n#     market_df = market_df.dropna(axis=0)\n    \n    return market_df\n\nmarket_train = feature_engineering(market_train_df, news_train_df)\nmarket_train",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dc2d7c427aa03c23160af16a3ef01241a093b127"
      },
      "cell_type": "code",
      "source": "# # Save universe data for latter use\nuniverse = market_train['universe']\ntime = market_train['time']\n\nprint(market_train.shape)\nup = market_train.returnsOpenNextMktres10 >= 0\n\nfcol = [c for c in market_train_df.columns if c not in ['assetCode', 'assetCodes', 'assetCodesLen', 'assetName', 'assetCodeT', 'volume_to_mean',\n                                             'firstCreated', 'headline', 'headlineTag', 'marketCommentary', 'provider', 'returnsOpenPrevRaw1_to_volume',\n                                             'returnsOpenNextMktres10', 'sourceId', 'subjects', 'time', 'time_x', 'universe','sourceTimestamp']]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2dbf5543e8852d7b9f47f5a3d91164ff2d659537"
      },
      "cell_type": "code",
      "source": "X = market_train[fcol]#.values\nup = up.values\ny = market_train.returnsOpenNextMktres10#.values",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a01a9b155f5e1786d2b63df8f877bd4a953d34ca"
      },
      "cell_type": "code",
      "source": "# Scaling of X values\nmins = np.min(X, axis=0)\nmaxs = np.max(X, axis=0)\nrng = maxs - mins\nX = 1 - ((maxs - X) / rng)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "70b65e89627884fec0540734086f3d8899b58447"
      },
      "cell_type": "code",
      "source": "n_train = int(X.shape[0] * 0.8)\n\nX_train, y_train = X.iloc[:n_train], y.iloc[:n_train]\nX_valid, y_valid = X.iloc[n_train:], y.iloc[n_train:]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dc4bbea88c9fc479a3f3fedec921decb2a17e5a5"
      },
      "cell_type": "code",
      "source": "# For valid data, keep only those with universe > 0. This will help calculate the metric\nu_valid = (universe.iloc[n_train:] > 0)\nt_valid = time.iloc[n_train:]\n\nX_valid = X_valid[u_valid]\ny_valid = y_valid[u_valid]\nt_valid = t_valid[u_valid]\ndel u_valid",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "7e2fbf20d075b20d7eacca81b2a4d85596458a50"
      },
      "cell_type": "code",
      "source": "# Creat lgb datasets\ntrain_cols = X.columns.tolist()\ncategorical_cols = [] # ['assetCode', 'assetName', 'dayofweek', 'month']\n\n# Note: y data is expected to be a pandas Series, as we will use its group_by function in `sigma_score`\ndtrain = lgb.Dataset(X_train.values, y_train, feature_name=train_cols, categorical_feature=categorical_cols, free_raw_data=False)\ndvalid = lgb.Dataset(X_valid.values, y_valid, feature_name=train_cols, categorical_feature=categorical_cols, free_raw_data=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "00c66e08f7089f37864bef7eaba44593cd676f6c"
      },
      "cell_type": "code",
      "source": "lgb_params = dict(\n    objective = 'regression_l1',\n    learning_rate = 0.1,\n    num_leaves = 127,\n    max_depth = -1,\n#     min_data_in_leaf = 1000,\n#     min_sum_hessian_in_leaf = 10,\n    bagging_fraction = 0.75,\n    bagging_freq = 2,\n    feature_fraction = 0.5,\n    lambda_l1 = 0.0,\n    lambda_l2 = 1.0,\n    metric = 'None', # This will ignore the loss objetive and use sigma_score instead,\n    seed = 42 # Change for better luck! :)\n)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "51ac32412e26d9fd7217875f71836100632a577a"
      },
      "cell_type": "code",
      "source": "evals_result = {}\nm = lgb.train(lgb_params, dtrain, num_boost_round=1000, valid_sets=(dvalid,), valid_names=('valid',), verbose_eval=25,\n              early_stopping_rounds=100, \n              feval=sigma_score, \n              evals_result=evals_result)\n\ndf_result = pd.DataFrame(evals_result['valid'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "12fff768ef4132ec89c8b7368a19cffe795c09ab"
      },
      "cell_type": "code",
      "source": "ax = df_result.plot(figsize=(12, 8))\nax.scatter(df_result['sigma_score'].idxmax(), df_result['sigma_score'].max(), marker='+', color='red')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d662aff7c5f4fb5ce5657ffeaa550719d54152dc"
      },
      "cell_type": "code",
      "source": "num_boost_round, valid_score = df_result['sigma_score'].idxmax()+1, df_result['sigma_score'].max()\nprint(lgb_params)\nprint(f'Best score was {valid_score:.5f} on round {num_boost_round}')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7048e091455aef6afb99ac9c6408a86cb564fca4"
      },
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 2, figsize=(14, 14))\nlgb.plot_importance(m, ax=ax[0])\nlgb.plot_importance(m, ax=ax[1], importance_type='gain')\nfig.tight_layout()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e93e8322d665d176e2b4ec46c787c0b1b12cccdc"
      },
      "cell_type": "code",
      "source": "# def make_predictions(predictions_template_df, market_obs_df, news_obs_df, le):\n#     market_obs_df = feature_engineering(market_obs_df, news_obs_df)\n#     predictions_template_df.confidenceValue = np.clip(model.predict(x), -1, 1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "5e747e9269d0702f177db078dea7b690e9fc3d03"
      },
      "cell_type": "code",
      "source": "# days = env.get_prediction_days()\n\n# for (market_obs_df, news_obs_df, predictions_template_df) in days:\n#     make_predictions(predictions_template_df, market_obs_df, news_obs_df, le)\n#     env.predict(predictions_template_df)\n# print('Done!')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3f010d5d3019d032d80cac0615056751b76678f7"
      },
      "cell_type": "code",
      "source": "# env.write_submission_file()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "232a774c51a05ce63c571d10cea12051dea000fe",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# days = env.get_prediction_days()\n# for (market_obs_df, news_obs_df, predictions_template_df) in days:\n#     print(market_obs_df.groupby('time').count())\n    \n# (1820, 5) Only the latest 2017-01-03",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "                       time           ...           returnsOpenPrevMktres10\n0 2017-01-03 22:00:00+00:00           ...                          0.001985\n1 2017-01-03 22:00:00+00:00           ...                               NaN\n2 2017-01-03 22:00:00+00:00           ...                         -0.015277\n3 2017-01-03 22:00:00+00:00           ...                          0.011201\n4 2017-01-03 22:00:00+00:00           ...                         -0.010078\n\n[5 rows x 14 columns]\nERROR: You must call `predict` before you can get the data for the next prediction day.\n",
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'NoneType' object is not iterable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-4237b5f5de31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_prediction_days\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmarket_obs_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnews_obs_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions_template_df\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdays\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarket_obs_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5bf81f80b596e3ed752e746cee3bcebbd16b5d00"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}